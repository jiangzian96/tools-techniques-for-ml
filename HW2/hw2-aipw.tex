%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[ruled]{article}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose}
\setcounter{secnumdepth}{5}
\usepackage{color}
\usepackage{enumitem}
\usepackage{algorithm2e}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[unicode=true,
 bookmarks=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=true]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newlength{\lyxlabelwidth}      % auxiliary length 

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\makeatother

\usepackage{listings}
\lstset{backgroundcolor={\color{white}},
basicstyle={\footnotesize\ttfamily},
breakatwhitespace=false,
breaklines=true,
captionpos=b,
commentstyle={\color{mygreen}},
deletekeywords={...},
escapeinside={\%*}{*)},
extendedchars=true,
frame=shadowbox,
keepspaces=true,
keywordstyle={\color{blue}},
language=Python,
morekeywords={*,...},
numbers=none,
numbersep=5pt,
numberstyle={\tiny\color{mygray}},
rulecolor={\color{black}},
showspaces=false,
showstringspaces=false,
showtabs=false,
stepnumber=1,
stringstyle={\color{mymauve}},
tabsize=2}
\begin{document}
\input{my_macros_paper.tex}
\title{Tools and Techniques for Machine Learning\\
Homework 2: Regression imputation, covariate shift, and control variates}

\maketitle
\textbf{Instructions}: Your answers to the questions below, including
plots and mathematical work, should be submitted as a single PDF file.
It's preferred that you write your answers using software that typesets
mathematics (e.g. \LaTeX , \LyX , or Jupyter), though if you need
to you may scan handwritten work. For submission, you can also export
your Jupyter notebook and merge that PDF with your PDF for the written
solutions into one file. \textbf{Don't forget to complete the Jupyter
notebook as well, for the programming part of this assignment}.

\section*{General hint on Adam's Law}

A couple times in this assignment we'll need a variant on the basic
Adam's Law. Adam's Law is that $\ex\left[\ex\left[Y\mid X\right]\right]=\ex Y$.
The variant we'll need is that Adam's Law still holds when everything
is conditioned on a particular event. For example, $\ex\left[\ex\left[Y\mid X,Z>a\right]\mid Z>a\right]=\ex\left[Y\mid Z>a\right]$.
We could see this by defining $\left(X',Y'\right)$ to have joint
distribution that's equal to the conditional distribution of $(X,Y)\mid Z>a$.
Then 
\begin{eqnarray*}
 &  & \ex\left[\ex\left[Y\mid X,Z>a\right]\mid Z>a\right]\\
 & = & \ex\left[\ex\left[Y'\mid X'\right]\right]=\ex\left[Y'\right]=\ex\left[Y\mid Z>a\right].
\end{eqnarray*}
 Another approach would be to define the random variable $W=\ind{Z>a}$.
Then
\[
\ex\left[\ex\left[Y\mid X,W\right]\mid W\right]=\ex\left[Y\mid W\right],
\]
by the generalized form of Adam's Law. This implies that
\[
\ex\left[\ex\left[Y\mid X,W=1\right]\mid W=1\right]=\ex\left[Y\mid W=1\right],
\]


\section{Complete case mean - unbiased or what?}

Let $R_{i}\in\left\{ 0,1\right\} $ be the response indicator, $Y_{i}\in\reals$
the response. Consider the MCAR setting, in which $R_{i}\indep Y_{i}$,
and suppose $(R,Y),(R_{1},Y_{1}),\ldots,(R_{n},Y_{n})$ are i.i.d.
We observe data $\cd=\left(\left(R_{1},R_{1}Y_{1}\right),\ldots,\left(R_{n},R_{n}Y_{n}\right)\right)$.
The complete case estimator is defined as
\[
\hat{\mu}_{\text{cc}}=\hat{\mu}_{\text{cc}}(\cd)=\frac{\sum_{i=1}^{n}R_{i}Y_{i}}{\sum_{i=1}^{n}R_{i}}.
\]
Note that if $R_{1}=\cdots=R_{n}=0$, then $\hat{\mu}_{\text{cc}}=\frac{0}{0}$,
which is undefined. Since $\hat{\mu}_{\text{cc}}$ is undefined with
nonzero probability, it doesn't have an expectation or bias. In this
problem, we consider whether $\hat{\mu}_{\text{cc}}$ is unbiased
after ruling out the case where it's undefined.
\begin{enumerate}
\item Show that $\ex\left[\hat{\mu}_{\text{cc}}\mid\sum_{i=1}^{n}R_{i}>0\right]=\ex Y$.
(Hint: Show that $\ex\left[\hat{\mu}_{\text{cc}}\mid R_{1},\ldots,R_{n},\sum_{i=1}^{n}R_{i}>0\right]=\ex Y$.)

\end{enumerate}

\section{Regression imputation with $\protect\ex\left[Y\mid X=x\right]$ }

Consider the MAR setting. In regression imputation, we fit a regression
function $\hat{f}(x)$ to the complete cases, and the regression imputation
estimator for $\ex Y$ is given by
\[
\hat{\mu}_{\hat{f}}=\frac{1}{n}\sum_{i=1}^{n}\left[R_{i}Y_{i}+\left(1-R_{i}\right)\hat{f}(X_{i})\right].
\]
There is also an alternative form of regression imputation where we
apply $\hat{f}(x)$ to all the $X_{i}$'s, not just the incomplete
cases. This estimator is given by
\[
\hat{\mu}_{\hat{f}\text{-full}}=\frac{1}{n}\sum_{i=1}^{n}\hat{f}(X_{i}).
\]
In this problem, we want to verify that, if we use $\ex\left[Y\mid X=x\right]$
for our regression imputation, our regression imputation estimators
are unbiased. This will give us some hope that, under the appropriate
technical conditions, if our model is well-specified, then regression
imputation is consistent.
\begin{enumerate}
\item If $f(x)=\ex\left[Y\mid X=x\right],$show that $\ex\left[\hat{\mu}_{f\text{-full}}\right]=\ex Y$.

\item If $f(x)=\ex\left[Y\mid X=x\right],$show that $\ex\left[\hat{\mu}_{f}\right]=\ex Y$.
(Hint: See the slide on ``Adam's Law / Law of iterated expectation''
for inspiration, and you'll also need to use the MAR assumption that
$Y_{i}\indep R_{i}\mid X_{i}$.)

 
\end{enumerate}


\section{A family of simple AIPW estimators}

(Continuing the ``IPW estimator is not equivariant'' problem (1.3)
in Homework \#1.)

Suppose $\cd$ represents the dataset $(X_{1},R_{1},R_{1}Y_{1}),\ldots,(X_{n},R_{n},R_{n}Y_{n})$
from a MAR setting. For any $a\in\reals$, we'll write $\cd+a$ for
the dataset $(X_{1},R_{1},R_{1}\left(Y_{1}+a\right)),\ldots,(X_{n},R_{n},R_{n}\left(Y_{n}+a\right))$,
which is the same as $\cd$, but with each $Y$ value shifted by $a$.
Recall the definition of the estimator $\hat{\mu}_{\ipw}(\cd)=\frac{1}{n}\sum_{i=1}^{n}\frac{R_{i}Y_{i}}{\pi(X_{i})}$.
For any $a\in\reals$, define a new estimator $\hat{\mu}_{\ipw-a}=\hat{\mu}_{\ipw-a}(\cd):=\hat{\mu}_{\ipw}(\cd+a)-a$.
In the last homework, we showed that $\hat{\mu}_{\ipw}(\cd+a)=\hat{\mu}_{\ipw}(\cd)+\frac{a}{n}\sum_{i=1}^{n}\frac{R_{i}}{\pi(X_{i})}$
and that $\ex\hat{\mu}_{\ipw-a}(\cd)=\ex Y$. 
\begin{enumerate}
\item We can view $\hat{\mu}_{\ipw-a}$ as an augmented IPW (AIPW) estimator
-{}- that is, as a control-variate adjusted IPW estimator. With this
view, what is the control variate and what is its expectation?

 
\item Given what we learned about control variates, how would you choose
$a\in\reals$? (There are many reasonable answers to this question,
and I don't believe there is a single best answer without additional
assumptions. That said, the section on ``Optimal scaling to improve
variance'' in the control variates module may be a source of some
ideas.)

\end{enumerate}

\section{Election forecasting}

Suppose we want to forecast the outcome of an election with two candidates.
We have a budget to call $n$ people and ask who they'll vote for.
Each individual $i$ is described by the following random variables:
\begin{eqnarray*}
X_{i}\in\cx &  & \text{covariates describing individual \ensuremath{i}}\\
T_{i}\in\left\{ 0,1\right\}  &  & \text{indicator for whether \ensuremath{i} will vote ("turnout indicator")}\\
R_{i}\in\left\{ 0,1\right\}  &  & \text{indicator for whether \ensuremath{i} will respond to a survey question if called}\\
Y_{i}\in\left\{ 0,1\right\}  &  & \text{indicator for which candidate an individual will vote for, if they vote}
\end{eqnarray*}
We'll assume the existence of an ``eligible voter generating distribution\footnote{In reality, there is a fixed set of potential voters. We're taking
the ``eligible voter generating distribution'' approach to align
more with the framework of our class. For large elections, the list
of all potential voters is so much larger than the size of the survey
sample that this is a very reasonable approximation.}'', and we'll refer to it as $P$. To carry out the survey, $n$
individuals are sampled from $P$. For individuals who respond (i.e.
for whom $R=1$), we will assume they reveal their true value of $Y$.
We'll write the full data corresponding to this scenario as
\[
\left(X,R,Y,T\right),\left(X_{1},R_{1},Y_{1},T_{1}\right),\ldots,\left(X_{n},R_{n},Y_{n},T_{n}\right),
\]
sampled i.i.d. from $P$. However, since we only observe $Y$ when
$R=1$, and we don't observe $T$ at all, we'll write the observed
data as 
\[
\left(X,R,RY\right),\left(X_{1},R_{1},R_{1}Y_{1}\right),\ldots,\left(X_{n},R_{n},R_{n}Y_{n}\right).
\]
\textbf{We'll make the following assumptions}:
\begin{enumerate}
\item $R$, $Y$, and $T$ are mutually independent given $X$. (In particular,
this implies $Y\indep R\mid X$ and $Y\indep T\mid X$.)
\item We have access to a function $\pi_{t}(x)=\pr\left(T=1\mid X=x\right)$
that gives the ``turnout probability'', i.e. the probability that
an individual will go vote, given their covariates\footnote{There are organizations and companies that produce this type of thing.
It's not a straightforward statistics or machine learning problem,
since it's not clear there are any high quality labels to fit a model
to. But we'll assume that somebody else has already solved this problem
for us.}.
\item We have access to a function $\pi_{r}(x)=\pr\left(R=1\mid X=x\right)$
that gives the ``response probability.'' This can be estimated using
the observed data with a simple probabilistic classifier, such as
logistic regression, but we'll also assume this part of the problem
has already been solved. 
\item Every voter has at least some chance of responding to a survey. To
put in mathematical terms: $\pi_{t}(x)>0\implies\pi_{r}(x)>0\quad\forall x\in\cx$. 
\end{enumerate}
To forecast the election, we want to estimate $\pr\left(Y=1\mid T=1\right)$,
i.e. the rate of voting for candidate $1$ among individuals who actually
go vote. In this problem, we'll use a variant of regression imputation
that accounts for the covariate shift between the survey respondent
distribution and the voter distribution.

\subsection{Fitting the regression}

If we fit a model to the survey responses (i.e. the complete cases,
i.e. the $\left(X_{i},Y_{i}\right)$ pairs corresponding to $R_{i}=1$)
in the usual way (say empirical risk minimization over some space
of functions), we'll end up with a function $\hat{f}(x)$ that has
low risk with respect to the distribution $p(x,y\mid R=1)$. In other
words, $\hat{f}(x)$ will perform well for survey responders, but
what we really need is for $\hat{f}(x)$ to perform well for voters,
i.e. to have low risk w.r.t. the distribution $p(x,y\mid T=1)$. 
\begin{enumerate}
\item If we're fitting $\hat{f}(x)$ to data from $p(x,y\mid R=1)$ (without
importance weighting), then we expect $\hat{f}(x)\approx\ex\left[Y\mid X=x,R=1\right]$.
And if we could fit $\hat{f}(x)$ to data from $p(x,y\mid T=1)$ then
we would have $\hat{f}(x)\approx\ex\left[Y\mid X=x,T=1\right]$. We've
assumed that $Y\indep T\mid X$ and $Y\indep R\mid X$, which implies
$\ex\left[Y\mid X=x,R=1\right]=\ex\left[Y\mid X=x,T=1\right]=\ex\left[Y\mid X=x\right]$.
Given this observation, one might suggest that importance weighting
doesn't make a difference: we're estimating $\ex\left[Y\mid X=x\right]$
no matter which data we're fitting on. Describe a circumstance when
this suggestion is reasonable, and a circumstance when it is not reasonable\@.
(Hint: model misspecification)

\item Give an appropriate importance-weighted empirical risk for $f(x)$
in terms of a loss function $\ell\left(f(X),Y\right)$ and the observed
data described above. We'll only use it for learning $\hat{f}$, so
don't worry about scale factors.

\end{enumerate}

\subsection{Using our regression to forecast the election}

The goal of this section is come up with an estimator for $\pr\left(Y=1\mid T=1\right)$.
\begin{enumerate}
\item Let $f(x)=\pr\left(Y=1\mid X=x,T=1\right)=\ex\left[Y\mid X=x,T=1\right]$.
Show that
\[
\pr\left(Y=1\mid T=1\right)=\frac{\ex\left[\pi_{t}(X)f(X)\right]}{\ex\left[\pi_{t}(X)\right]}.
\]
You can follow your own path, or use the steps in the subproblems
below.
\begin{enumerate}
\item Show that $\pr(Y=1\mid T=1)=\ex\left[f(X)\mid T=1\right]$. 

\item Show that $\ex\left[Tf(X)\right]=\pr\left(T=1\right)\ex\left[f(X)\mid T=1\right]$.
(Hint Remember that $T\in\left\{ 0,1\right\} $.)

\item Use the previous two results to show that $\pr(Y=1\mid T=1)=\ex\left[\pi_{t}(X)f(X)\right]/\pr\left(T=1\right)$.
(Hint: $\pi_{t}(X)=\ex\left[T\mid X\right]$.)

 
\item Conclude the proof of this section by showing that $\pr\left(T=1\right)=\ex\left[\pi_{t}(X)\right]$.

\end{enumerate}
\item Propose an estimator for $\pr\left(Y=1\mid T=1\right)$ that uses
our estimated regression function $\hat{f}(x)$, together with $\pi_{t}(x)$,
$\pi_{r}(x)$, and a new large sample\footnote{In this context, getting samples of just covariates $X$ is generally
cheap compared to getting samples of $(X,Y)$ pairs.} $X_{1},\ldots,X_{N}$ of covariates from $P$. Your estimator should
converge to $\frac{\ex\left[\pi_{t}(X)\hat{f}(X)\right]}{\ex\left[\pi_{t}(X)\right]}$
as $N\to\infty$, though proving this is optional.

\end{enumerate}

\end{document}
