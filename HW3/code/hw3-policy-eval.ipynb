{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3: Offline Policy Value Estimation (i.e. Counterfactual evaluation)\n",
    "\n",
    "### Introduction\n",
    "In this lab, we're going to be reproducing a few results from http://proceedings.mlr.press/v97/vlassis19a.html, and extending their results in a few ways.  Here's an overview: We start by taking a multiclass classification problem and splitting it into train and test.  There are 26 classes, which we'll interpret as 26 possible actions to take for every input context. On the training set, we fit a multinomial logistic regression model to predict the correct label/best action.  Following the paper, we create a logging policy based on this model (details supplied in the relevant spot below).  We then generate \"logged bandit feedback\" for this logging policy using the **test** set.  Given this logged bandit feedback, we'll try out several different methods for estimating the value of various policies.  We'll also estimate the value of each of these policies using the full-feedback (i.e. the full observed rewards), and we'll treat that as the ground truth value for the purpose of performance assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeCV, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_integer_dtype\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.special import expit\n",
    "import seaborn as sns\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fully_observed_bandit():\n",
    "    \"\"\"\n",
    "    This loads in a multiclass classification problem and reformulates it as a fully observed bandit problem.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_l = pd.read_csv('data/letter-recognition.data',\n",
    "                       names = ['a']+[f'x{i}' for i in range(16)])\n",
    "    X = df_l.drop(columns=['a'])\n",
    "\n",
    "    # Convert labels to ints and one-hot\n",
    "    y = df_l['a']\n",
    "    # if y is not column of integers (that represent classes), then convert\n",
    "    if not is_integer_dtype(y.dtype):\n",
    "        y = y.astype('category').cat.codes\n",
    "\n",
    "    ## Full rewards\n",
    "    n = len(y)\n",
    "    k = max(y)+1\n",
    "    full_rewards = np.zeros([n, k])\n",
    "    full_rewards[np.arange(0,n),y] = 1\n",
    "    contexts = X\n",
    "    best_actions = y\n",
    "    return contexts, full_rewards, best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 actions, the context space is 16 dimensional, and there are 20000 examples.\n",
      "For example, the first item has context vector:\n",
      "   x0  x1  x2  x3  x4  x5  x6  x7  x8  x9  x10  x11  x12  x13  x14  x15\n",
      "0   2   8   3   5   1   8  13   0   6   6   10    8    0    8    0    8.\n",
      "The best action is 19.  The reward for that action is 1 and all other actions get reward 0.\n",
      "The reward information is store in full_rewards as the row\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0.].\n"
     ]
    }
   ],
   "source": [
    "contexts, full_rewards, best_actions = get_fully_observed_bandit()\n",
    "n, k = full_rewards.shape\n",
    "_, d = contexts.shape\n",
    "print(f\"There are {k} actions, the context space is {d} dimensional, and there are {n} examples.\")\n",
    "print(f\"For example, the first item has context vector:\\n{contexts.iloc[0:1]}.\")\n",
    "print(f\"The best action is {best_actions[0]}.  The reward for that action is 1 and all other actions get reward 0.\")\n",
    "print(f\"The reward information is store in full_rewards as the row\\n{full_rewards[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose train/test indices\n",
    "rng = default_rng(7)\n",
    "train_frac = 0.5\n",
    "train_size = round(train_frac * n)\n",
    "train_idx = rng.choice(n, size = train_size, replace = False)\n",
    "test_idx = np.setdiff1d(np.arange(n), train_idx, assume_unique=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies\n",
    "In this section, we'll build out a Policy class, some specific policies, and evaluate policies on full-feedback data.\n",
    "\n",
    "**Problem 1.** Complete the Policy class and the UniformActionPolicy classes below. Run the code provided to get an estimate of the value of the uniform action policy using the test set.  Explain why the value you get makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, num_actions=2):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_action_distribution(self, X):\n",
    "        \"\"\"   \n",
    "        This method is intended to be overridden by each implementation of Policy.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts\n",
    "\n",
    "        Returns:\n",
    "            2-dim numpy array with the same number of rows as X and self.num_actions columns. \n",
    "                Each rows gives the policy's probability distribution over actions conditioned on the context in the corresponding row of X\n",
    "        \"\"\"   \n",
    "        raise NotImplementedError(\"Must override method\")\n",
    "\n",
    "    def get_action_propensities(self, X, actions):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions\n",
    "            actions (np.array): actions taken, represented by integers, corresponding to rows of X\n",
    "\n",
    "        Returns:\n",
    "            1-dim numpy array of probabilities (same size as actions) for taking each action in its corresponding context\n",
    "        \"\"\"   \n",
    "        ## DONE\n",
    "        action_distribution = self.get_action_distribution(X)\n",
    "        return np.take_along_axis(action_distribution, actions.reshape(-1, 1), axis=1).flatten()\n",
    "\n",
    "    def select_actions(self, X, rng=default_rng(1)):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions and propensities returned\n",
    "\n",
    "        Returns:\n",
    "            actions (np.array): 1-dim numpy array of length equal to the number of rows of X.  Each entry is an integer indicating the action selected for the corresponding context in X. \n",
    "                The action is selected randomly according to the policy, conditional on the context specified in the appropriate row of X.\n",
    "            propensities (np.array): 1-dim numpy array of length equal to the number of rows of X; gives the propensity for each action selected in actions\n",
    "\n",
    "        \"\"\"   \n",
    "        ## DONE\n",
    "        action_distribution = self.get_action_distribution(X)\n",
    "        actions = np.array([np.random.choice(26, 1, p=action_distribution[i]) for i in range(X.shape[0])]).flatten()\n",
    "        propensities = self.get_action_propensities(X, actions)\n",
    "        assert len(actions) == len(propensities) == X.shape[0]\n",
    "        \n",
    "        return actions, propensities\n",
    "        \n",
    "        \n",
    "    def get_value_estimate(self, X, full_rewards):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of full_rewards\n",
    "            full_rewards (np.array): 2-dim numpy array with the same number of rows as X and self.num_actions columns; \n",
    "                each row gives the rewards that would be received for each action for the context in the corresponding row of X.\n",
    "                This would only be known in a full-feedback bandit, or estimated in a direct method\n",
    "\n",
    "        Returns:\n",
    "            scalar value giving the expected average reward received for playing the policy for contexts X and the given full_rewards\n",
    "\n",
    "        \"\"\"   \n",
    "        ## DONE\n",
    "        n = X.shape[0]\n",
    "        actions, propensities = self.select_actions(X)\n",
    "        action_distribution = self.get_action_distribution(X)\n",
    "        \n",
    "        return (full_rewards*action_distribution).sum()/n\n",
    "\n",
    "\n",
    "class UniformActionPolicy(Policy):\n",
    "    def __init__(self, num_actions=2):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        ## DONE\n",
    "        return np.full((X.shape[0], self.num_actions), 1.0/self.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate of the value of the uniform action policy using the full-feedback test set is 0.038461538461538484.\n"
     ]
    }
   ],
   "source": [
    "X_train = contexts.iloc[train_idx].to_numpy()\n",
    "y_train = best_actions.iloc[train_idx].to_numpy()\n",
    "X_test = contexts.iloc[test_idx].to_numpy()\n",
    "y_test = best_actions.iloc[test_idx].to_numpy()\n",
    "full_rewards_test = full_rewards[test_idx]\n",
    "\n",
    "uniform_policy = UniformActionPolicy(num_actions=k)\n",
    "uniform_policy_value = uniform_policy.get_value_estimate(X=X_test, full_rewards=full_rewards_test)\n",
    "print(f\"The estimate of the value of the uniform action policy using the full-feedback test set is {uniform_policy_value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimate of the value of this uniform action policy is $\\frac{1}{26}$. This makes sense because as the formulation of the value estimate suggests, $$V(\\pi) = \\frac{1}{n}\\sum_{i=1}^{n}\\sum_{a=1}^{k}[r(X_i, a)\\pi(a|X_i)],$$ in the inner sum every propensity score is $\\frac{1}{26}$ and only $1$ action out of $26$ has the reward $1$ while the rest is $0$. Thus, the inner sum is essentially $\\frac{1}{26}$. We add $\\frac{1}{26}$ up for $n$ times and take the average, which results in $\\frac{1}{26}$ as value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.**  Complete the SKLearnPolicy class below and run the code that creates two policies and estimates their values using the full reward information in the test set.  You should find that the deterministic policy has a higher value than the stochastic policy.  Nevertheless, why might one choose to deploy the stochastic policy rather than the deterministic policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic policy true value 0.6261601174415055.\n",
      "Deterministic policy true value 0.7631.\n"
     ]
    }
   ],
   "source": [
    "## Develop more policies\n",
    "\n",
    "class SKLearnPolicy(Policy):\n",
    "    \"\"\" \n",
    "    An SKLearnPolicy uses a scikit learn model to generate an action distribution.  If the SKLearnPolicy is built with is_deterministic=False, \n",
    "    then the predict distribution for a context x should be whatever predict_proba for the model returns.  If is_deterministic=True, then all the probability mass \n",
    "    should be concentrated on whatever predict of the model returns.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_actions=2, is_deterministic=False):\n",
    "        self.is_deterministic = is_deterministic\n",
    "        self.num_actions = num_actions\n",
    "        self.model = model\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        ## DONE\n",
    "        if (self.is_deterministic):\n",
    "            predictions = self.model.predict(X)\n",
    "            return np.eye(self.num_actions)[predictions.reshape(-1)] # one hot\n",
    "        else:\n",
    "            return self.model.predict_proba(X)\n",
    "\n",
    "\n",
    "    def select_actions(self, X, rng=default_rng(1)):\n",
    "        ## DONE\n",
    "        if (self.is_deterministic):\n",
    "            actions = self.model.predict(X)\n",
    "            propensities = np.full(len(actions), 1.0)\n",
    "            return actions, propensities\n",
    "        else:\n",
    "            actions, propensities = Policy.select_actions(self, X)\n",
    "            return actions, propensities\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial')\n",
    "model.fit(X_train, y_train)\n",
    "policy_stochastic = SKLearnPolicy(model=model, num_actions=k, is_deterministic=False)\n",
    "policy_deterministic = SKLearnPolicy(model=model, num_actions=k, is_deterministic=True)\n",
    "\n",
    "policy_stochastic_true_value = policy_stochastic.get_value_estimate(X_test, full_rewards_test)\n",
    "policy_deterministic_true_value = policy_deterministic.get_value_estimate(X_test, full_rewards_test)\n",
    "print(f\"Stochastic policy true value {policy_stochastic_true_value}.\")\n",
    "print(f\"Deterministic policy true value {policy_deterministic_true_value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know our training data is biased towards $\\pi_0$, the deterministic policy may also be biased and offeres no exploration. On the other hand the stochastic policy offers exploration by randomly selecting actions based on the action distribution trained from data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.** Fill in the VlassisLoggingPolicy class below, and evaluate the value of this logging policy using the code provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate of the value of the logging policy using the full-feedback test set is 0.04715720000000002.\n"
     ]
    }
   ],
   "source": [
    "class VlassisLoggingPolicy(Policy):\n",
    "    \"\"\"\n",
    "    This policy derives from another deterministic policy following the recipe described in the Vlassis et al paper, on the top of the second column in section 5.3.\n",
    "    For any context x, if the deterministic policy selects action a, then this policy selects action a with probability eps (a supplied parameter), and spreads the\n",
    "    rest of the probability mass uniformly over the other actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, deterministic_target_policy, num_actions=2, eps=0.05):\n",
    "        self.num_actions = num_actions\n",
    "        self.target_policy = deterministic_target_policy\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        rest = (1.0-self.eps)/(self.num_actions-1)\n",
    "        actions, propensities = self.target_policy.select_actions(X)\n",
    "        action_distribution = np.eye(self.num_actions)[actions.reshape(-1)]*self.eps\n",
    "        action_distribution[action_distribution == 0.0] = rest\n",
    "        return action_distribution\n",
    "        \n",
    "    \n",
    "logging_policy = VlassisLoggingPolicy(policy_deterministic, num_actions=k, eps=0.05)\n",
    "logging_policy_value = logging_policy.get_value_estimate(X=X_test, full_rewards=full_rewards_test)\n",
    "print(f\"The estimate of the value of the logging policy using the full-feedback test set is {logging_policy_value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate bandit feedback and on-policy evaluation\n",
    "**Problem 4.** Take a look at the generate_bandit_feedback function, so you understand how it works.  Then generate bandit feedback using the test data -- generate as many rounds are there are contexts in the test data. Use the result to generate an \"on-policy\" estimate of the value of the logging policy.  How does it compare to our \"ground truth\" estimate you found previously using the full-feedback test set? Repeat using 1/100th, 1/10th, and 10x as much bandit feedback, to see how much the value estimates change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bandit_feedback(contexts, full_rewards, policy,\n",
    "                             new_n = None,\n",
    "                             rng=default_rng(1)):\n",
    "    \"\"\"   \n",
    "    Args:\n",
    "        contexts (np.array): contexts, rows correspond to entries of rewards\n",
    "        full_rewards (np.array): 2-dim numpy array with the same number of rows as X and number of columns corresponding to the number actions\n",
    "            each row gives the reward that would be received for each action for the context in the corresponding row of X. \n",
    "\n",
    "    Returns:\n",
    "        new_contexts (np.array): new_n rows and same number of columns as in contexts\n",
    "        actions (np.array): vector with new_n entries giving actions selected by the provided policy for the contexts in new_contexts\n",
    "        observed_rewards (np.array): vector with new_n entries giving actions selected by the provided policy for the contexts in new_contexts \n",
    "    \"\"\"   \n",
    "    \n",
    "    if new_n is None:\n",
    "        new_n = contexts.shape[0]\n",
    "    n, k = full_rewards.shape\n",
    "    num_repeats = np.ceil(new_n / n).astype(int)\n",
    "    new_contexts = np.tile(contexts, [num_repeats,1])\n",
    "    new_contexts = new_contexts[0:new_n]\n",
    "    new_rewards = np.tile(full_rewards, [num_repeats,1])\n",
    "    new_rewards = new_rewards[0:new_n]\n",
    "    actions, propensities = policy.select_actions(X=new_contexts, rng=rng)\n",
    "    observed_rewards = new_rewards[np.arange(new_n), actions]\n",
    "    return new_contexts, actions, observed_rewards, propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = full_rewards_test.shape[0]\n",
    "vals = []\n",
    "\n",
    "for n_ in [n/100, n/10, n, 10*n]:\n",
    "    new_contexts, actions, observed_rewards, propensities = generate_bandit_feedback(X_test, full_rewards_test,\\\n",
    "                                                                logging_policy, new_n=int(n_), rng=default_rng(1))\n",
    "    vals.append(sum(observed_rewards*propensities)/n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x203a58179a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([n/100, n/10, n, 10*n], vals, \"--o\", label=\"bandit feedback\")\n",
    "plt.xlabel(\"amount of bandit feedback\")\n",
    "plt.ylabel(\"Value estimates\")\n",
    "plt.hlines(logging_policy_value, xmin=0, xmax=100000, label=\"full feedback\", color=\"r\")\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with more bandit feedback, the higher the value estimate will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out off-policy value estimators\n",
    "**Problem 5.** Complete the get_value_estimators function below, per the specification.  Include the following estimators\n",
    "- Unweighted mean (done for you)\n",
    "- Importance-weighted (IW) value estimator\n",
    "- Self-normalized IW mean\n",
    "- Direct method with linear ridge regression reward predictor fit for each action\n",
    "- Direct method with IW-linear ridge regression reward predictor fit for each action\n",
    "- [Optional (not for credit)] Direct method with a non-linear reward predictor fit for each action\n",
    "- [Optional (not for credit)] Direct method with a non-linear reward predictor fit for all actions at once (action becomes part of the input)\n",
    "\n",
    "Run the code below that will apply your value estimators to a policy on logged bandit feedback. Verify that your results are reasonable. (Don't worry if your numbers are not a very close match for the results in the table.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build our value estimators\n",
    "\n",
    "def get_value_estimators(policy, contexts, actions, rewards, propensities, skip_slow_stuff=False):\n",
    "    \"\"\"   \n",
    "    Args:\n",
    "        policy (Policy): the policy we want to get a value estimate for\n",
    "        contexts (np.array): contexts from bandit feedback\n",
    "        actions (np.array): actions chosen for bandit feedback\n",
    "        rewards (np.array): rewards received in bandit feedback\n",
    "        propensities (np.array): the propensity for each action selected under the logging policy (which is not provided to this function)\n",
    "        skip_slow_stuff (boolean): boolean flag which allows you to turn on/off some slow estimators (ignore this if you like)\n",
    "    Returns:\n",
    "        est (dict): keys are string describing the value estimator, values are the corresponding value estimates \n",
    "    \"\"\"   \n",
    "\n",
    "    est = {}\n",
    "    est[\"mean\"] = np.mean(rewards)\n",
    "    \n",
    "    ## DONE\n",
    "    pi_w = policy.get_action_propensities(contexts, actions)\n",
    "    \n",
    "    assert len(rewards) == len(pi_w) == len(propensities)\n",
    "    importance_weights = pi_w/propensities\n",
    "    \n",
    "    est[\"IW\"] = np.mean(rewards*importance_weights)\n",
    "    est[\"SNIW\"] = np.sum(rewards*importance_weights) / np.sum(importance_weights)\n",
    "    \n",
    "    if not skip_slow_stuff:\n",
    "        models = []\n",
    "        for i in range(26):\n",
    "            idx = actions == i\n",
    "            X = contexts[idx]\n",
    "            y = rewards[idx]\n",
    "            model = Ridge()\n",
    "            model.fit(X, y)\n",
    "            models.append(model)\n",
    "            \n",
    "        n = len(actions)\n",
    "        res = 0.0\n",
    "        for i in range(n):\n",
    "            X_i = contexts[i].reshape(1, -1)\n",
    "            all_action_propensities = policy.get_action_propensities(X_i, np.arange(26))\n",
    "            predicted_rewards = []\n",
    "            for a in range(26):\n",
    "                model = models[a]\n",
    "                predicted_reward = model.predict(X_i)[0]\n",
    "                predicted_rewards.append(predicted_reward)\n",
    "            res += sum(all_action_propensities * np.asarray(predicted_rewards))\n",
    "        est[\"Ridge\"] = res/n\n",
    "        \n",
    "        \n",
    "        models = []\n",
    "        for i in range(26):\n",
    "            idx = actions == i\n",
    "            X = contexts[idx]\n",
    "            y = rewards[idx]\n",
    "            p_0 = propensities[idx]\n",
    "            p_w = policy.get_action_propensities(X, np.full(X.shape[0], i, dtype=int))\n",
    "            sample_weights = p_w/p_0\n",
    "            model = Ridge()\n",
    "            model.fit(X, y, sample_weights)\n",
    "            models.append(model)\n",
    "            \n",
    "        n = len(actions)\n",
    "        res = 0.0\n",
    "        for i in range(n):\n",
    "            X_i = contexts[i].reshape(1, -1)\n",
    "            propensities = policy.get_action_propensities(X_i, np.arange(26))\n",
    "            predicted_rewards = []\n",
    "            for a in range(26):\n",
    "                model = models[a]\n",
    "                predicted_reward = model.predict(X_i)[0]\n",
    "                predicted_rewards.append(predicted_reward)\n",
    "            res += sum(propensities*np.asarray(predicted_rewards))\n",
    "        est[\"Ridge_IW\"] = res/n\n",
    "        \n",
    "    return est\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator_stats(estimates, true_parameter_value=None):\n",
    "    \"\"\"\n",
    " \n",
    "     Args:\n",
    "        estimates (pd.DataFrame): each row corresponds to collection of estimates for a sample and\n",
    "            each column corresponds to an estimator\n",
    "        true_parameter_value (float): the true parameter value that we will be comparing estimates to\n",
    "            \n",
    "    Returns:\n",
    "        pd.Dataframe where each row represents data about a single estimator\n",
    "    \"\"\"\n",
    "    \n",
    "    est_stat = []\n",
    "    for est in estimates.columns:\n",
    "        pred_means = estimates[est]\n",
    "        stat = {}\n",
    "        stat['stat'] = est\n",
    "        stat['mean'] = np.mean(pred_means)\n",
    "        stat['SD'] = np.std(pred_means)\n",
    "        stat['SE'] = np.std(pred_means) / np.sqrt(len(pred_means))\n",
    "        if true_parameter_value:\n",
    "            stat['bias'] = stat['mean'] - true_parameter_value\n",
    "            stat['RMSE'] = np.sqrt(np.mean((pred_means - true_parameter_value) ** 2))\n",
    "        est_stat.append(stat)\n",
    "\n",
    "    return pd.DataFrame(est_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy true value 0.7631.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.0482,\n",
       " 'IW': 0.816,\n",
       " 'SNIW': 0.783109404990403,\n",
       " 'Ridge': 0.27112898219944515,\n",
       " 'Ridge_IW': 0.7623822729000462}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts_test, actions_test, rewards_test, propensities_test = generate_bandit_feedback(contexts=X_test, full_rewards=full_rewards_test, policy=logging_policy, rng=default_rng(6))\n",
    "policy = policy_deterministic\n",
    "est = get_value_estimators(policy, contexts_test, actions_test, rewards_test, propensities_test)\n",
    "policy_true_value = policy.get_value_estimate(X_test, full_rewards_test)\n",
    "print(f\"policy true value {policy_true_value}.\")\n",
    "df = pd.DataFrame(est, index=[0])\n",
    "est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6.** Run the code below to test your value estimators across multiple trials.  Write a few sentences about anything you learned from these experiments or that you find interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       stat      mean        SD        SE      bias      RMSE\n",
      "0      mean  0.046790  0.002009  0.000449 -0.716310  0.716313\n",
      "1        IW  0.762100  0.030799  0.006887 -0.001000  0.030815\n",
      "2      SNIW  0.762522  0.012827  0.002868 -0.000578  0.012841\n",
      "3     Ridge  0.266487  0.009019  0.002017 -0.496613  0.496695\n",
      "4  Ridge_IW  0.791159  0.031913  0.007136  0.028059  0.042494\n"
     ]
    }
   ],
   "source": [
    "trials=20\n",
    "val_ests = []\n",
    "policy = policy_deterministic\n",
    "policy_true_value = policy.get_value_estimate(X_test, full_rewards_test)\n",
    "rng=default_rng(6)\n",
    "for i in range(trials):\n",
    "    contexts, actions, rewards, propensities = generate_bandit_feedback(X_test, full_rewards_test, logging_policy, rng=rng)\n",
    "    est = get_value_estimators(policy, contexts, actions, rewards, propensities)\n",
    "    val_ests.append(est)\n",
    "\n",
    "df = pd.DataFrame(val_ests)\n",
    "print(get_estimator_stats(df, true_parameter_value=policy_true_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       stat      mean        SD        SE      bias      RMSE\n",
      "0      mean  0.047580  0.001715  0.000384 -0.578580  0.578583\n",
      "1        IW  0.634605  0.022671  0.005069  0.008445  0.024192\n",
      "2      SNIW  0.628373  0.012770  0.002856  0.002213  0.012961\n",
      "3     Ridge  0.254693  0.007423  0.001660 -0.371467  0.371541\n",
      "4  Ridge_IW  0.610243  0.016766  0.003749 -0.015918  0.023118\n"
     ]
    }
   ],
   "source": [
    "trials=20\n",
    "val_ests = []\n",
    "policy = policy_stochastic\n",
    "policy_true_value = policy.get_value_estimate(X_test, full_rewards_test)\n",
    "rng=default_rng(6)\n",
    "for i in range(trials):\n",
    "    contexts, actions, rewards, propensities = generate_bandit_feedback(X_test, full_rewards_test, logging_policy, rng=rng)\n",
    "    est = get_value_estimators(policy, contexts, actions, rewards, propensities)\n",
    "    val_ests.append(est)\n",
    "\n",
    "df = pd.DataFrame(val_ests)\n",
    "print(get_estimator_stats(df, true_parameter_value=policy_true_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       stat      mean        SD        SE      bias      RMSE\n",
      "0      mean  0.047705  0.002322  0.000519  0.009243  0.009531\n",
      "1        IW  0.038915  0.001803  0.000403  0.000454  0.001859\n",
      "2      SNIW  0.038921  0.001826  0.000408  0.000460  0.001883\n",
      "3     Ridge  0.044560  0.001578  0.000353  0.006099  0.006299\n",
      "4  Ridge_IW  0.038709  0.001403  0.000314  0.000247  0.001425\n"
     ]
    }
   ],
   "source": [
    "trials=20\n",
    "val_ests = []\n",
    "policy = uniform_policy\n",
    "policy_true_value = policy.get_value_estimate(X_test, full_rewards_test)\n",
    "rng=default_rng(6)\n",
    "for i in range(trials):\n",
    "    contexts, actions, rewards, propensities = generate_bandit_feedback(X_test, full_rewards_test, logging_policy, rng=rng)\n",
    "    est = get_value_estimators(policy, contexts, actions, rewards, propensities)\n",
    "    val_ests.append(est)\n",
    "\n",
    "df = pd.DataFrame(val_ests)\n",
    "print(get_estimator_stats(df, true_parameter_value=policy_true_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see across the 3 policies, uniform policy has a very low value. Stochastic and deterministic policies are more preferable. Also, importance weighting is very important due to the selection bias; this is true in both reward imputation and importance weighting. Of course, they are all better than the naive unweighted mean estimate. Also, using self-normalized weights on top of importance weighting gives the best result generally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
